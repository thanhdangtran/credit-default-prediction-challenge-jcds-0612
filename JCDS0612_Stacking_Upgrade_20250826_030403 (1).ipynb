{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a23336fc",
   "metadata": {},
   "source": [
    "Cell 1/11 â€” Imports & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de21af55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing libs (if any): None\n",
      "Paths: ./train.csv ./test.csv ./sample_submission.csv\n",
      "Outdir: ./outputs\n"
     ]
    }
   ],
   "source": [
    "# ============================== CELL 1/11: Imports & Config ==============================\n",
    "import os, json, time, random\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Try to import GBM libraries; skip gracefully if missing\n",
    "_MISSING_LIBS = []\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "except Exception:\n",
    "    lgb = None\n",
    "    _MISSING_LIBS.append(\"lightgbm\")\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "except Exception:\n",
    "    xgb = None\n",
    "    _MISSING_LIBS.append(\"xgboost\")\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostClassifier, Pool\n",
    "except Exception:\n",
    "    CatBoostClassifier = None\n",
    "    Pool = None\n",
    "    _MISSING_LIBS.append(\"catboost\")\n",
    "\n",
    "# ===== Quick Config (edit these) =====\n",
    "TRAIN_CSV = \"./train.csv\"            # same folder as the notebook\n",
    "TEST_CSV = \"./test.csv\"              # same folder as the notebook\n",
    "SAMPLE_SUB = \"./sample_submission.csv\"\n",
    "OUTDIR = \"./outputs\"                 # output directory will be created if it doesn't exist\n",
    "SEEDS = [42, 2021, 7, 99, 1234]      # change/add seeds for more stability (takes longer)\n",
    "N_FOLDS = 10                         # AUC-friendly CV; keep stratified\n",
    "\n",
    "print(\"Missing libs (if any):\", \", \".join(_MISSING_LIBS) or \"None\")\n",
    "print(\"Paths:\", TRAIN_CSV, TEST_CSV, SAMPLE_SUB)\n",
    "print(\"Outdir:\", OUTDIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fc28ca",
   "metadata": {},
   "source": [
    "Cell 2/11 â€” Utilities nhá»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a22e82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================== CELL 2/11: Small Utilities ==============================\n",
    "\"\"\"Helper functions: time stamp, AUC, RNG seed, mkdir.\"\"\"\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "def timestamp() -> str:\n",
    "    return datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "def auc(y_true, y_prob) -> float:\n",
    "    return roc_auc_score(y_true, y_prob)\n",
    "\n",
    "def safe_makedirs(path: str):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01179ab2",
   "metadata": {},
   "source": [
    "Cell 3/11 â€” Khai bÃ¡o nhÃ³m cá»™t cho FE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "349e8a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================== CELL 3/11: FE Constants ==============================\n",
    "\"\"\"Define column groups used by feature engineering. No need to change unless your schema differs.\"\"\"\n",
    "BILL_COLS = [f\"BILL_AMT{i}\" for i in range(1, 7)]\n",
    "PAY_AMT_COLS = [f\"PAY_AMT{i}\" for i in range(1, 7)]\n",
    "PAY_STATUS_COLS = [\"PAY_0\", \"PAY_2\", \"PAY_3\", \"PAY_4\", \"PAY_5\", \"PAY_6\"]\n",
    "CAT_COLS = [\"SEX\", \"EDUCATION\", \"MARRIAGE\"]\n",
    "TARGET_COL = \"default_payment_next_month\"\n",
    "ID_COL = \"ID\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2aa0d6",
   "metadata": {},
   "source": [
    "Cell 4/11 â€” FE Helpers (stats, slope, diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e84382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================== CELL 4/11: FE Helpers ==============================\n",
    "\"\"\"Row-wise statistics, slope, and difference helpers used by add_safe_features.\"\"\"\n",
    "\n",
    "def _rowwise_stats(df: pd.DataFrame, cols: List[str], prefix: str) -> pd.DataFrame:\n",
    "    X = df[cols].astype(float).values\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "    out[f\"{prefix}_sum\"] = X.sum(axis=1)\n",
    "    out[f\"{prefix}_mean\"] = X.mean(axis=1)\n",
    "    out[f\"{prefix}_std\"] = X.std(axis=1)\n",
    "    out[f\"{prefix}_min\"] = X.min(axis=1)\n",
    "    out[f\"{prefix}_max\"] = X.max(axis=1)\n",
    "    out[f\"{prefix}_last\"] = X[:, -1]\n",
    "    return out\n",
    "\n",
    "def _slope_over_time(df_vals: np.ndarray, x: np.ndarray) -> np.ndarray:\n",
    "    xm = x.mean()\n",
    "    denom = ((x - xm) ** 2).sum()\n",
    "    y = df_vals\n",
    "    ym = y.mean(axis=1, keepdims=True)\n",
    "    num = ((x - xm) * (y - ym)).sum(axis=1)\n",
    "    return num / (denom + 1e-9)\n",
    "\n",
    "def _differences_stats(df_vals: np.ndarray, prefix: str) -> Dict[str, np.ndarray]:\n",
    "    diffs = np.diff(df_vals, axis=1)\n",
    "    return {\n",
    "        f\"{prefix}_diff_mean\": diffs.mean(axis=1),\n",
    "        f\"{prefix}_diff_std\": diffs.std(axis=1),\n",
    "        f\"{prefix}_diff_min\": diffs.min(axis=1),\n",
    "        f\"{prefix}_diff_max\": diffs.max(axis=1),\n",
    "        f\"{prefix}_diff_last\": diffs[:, -1],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf008fb",
   "metadata": {},
   "source": [
    "Cell 5/11 â€” HÃ m FE chÃ­nh: add_safe_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "518e1cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================== CELL 5/11: add_safe_features() ==============================\n",
    "\"\"\"Main FE function. Safe, row-wise, no leakage. Month-wise alignment for ratios/shortfalls.\"\"\"\n",
    "\n",
    "import re\n",
    "\n",
    "def _get_months(df, prefix):\n",
    "    # Láº¥y cÃ¡c cá»™t cÃ³ dáº¡ng PREFIX + sá»‘, vÃ­ dá»¥: BILL_AMT1..6\n",
    "    months = []\n",
    "    for c in df.columns:\n",
    "        if c.startswith(prefix):\n",
    "            m = re.search(r'(\\d+)$', c)\n",
    "            if m:\n",
    "                months.append(int(m.group(1)))\n",
    "    return sorted(set(months))\n",
    "\n",
    "def add_safe_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Chuáº©n hoÃ¡ nháº¹ Ä‘á» phÃ²ng input báº©n\n",
    "    df = df.copy()\n",
    "    df.columns = df.columns.str.strip()\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "    \n",
    "    out = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # Basic\n",
    "    out[\"LIMIT_BAL_log\"] = np.log1p(df[\"LIMIT_BAL\"].astype(float))\n",
    "\n",
    "    # Age (numeric + binned)\n",
    "    age = df[\"AGE\"].astype(float)\n",
    "    out[\"AGE\"] = age\n",
    "    age_bins = pd.cut(age, bins=[0, 25, 35, 45, 55, 65, 100], labels=False, include_lowest=True)\n",
    "    out[\"AGE_BIN\"] = age_bins.fillna(-1).astype(int)\n",
    "\n",
    "    # ===== Determine available months dynamically (robust to 6 or 12 months, etc.) =====\n",
    "    bill_months = _get_months(df, \"BILL_AMT\")\n",
    "    pay_months  = _get_months(df, \"PAY_AMT\")\n",
    "    months      = [m for m in bill_months if m in pay_months]   # giao nhau Ä‘á»ƒ Ä‘áº£m báº£o match\n",
    "    if len(months) == 0:\n",
    "        # fallback chuáº©n 6 thÃ¡ng\n",
    "        months = [1,2,3,4,5,6]\n",
    "\n",
    "    # Build aligned BILL and PAY frames with the same months\n",
    "    bill_cols = [f\"BILL_AMT{m}\" for m in months]\n",
    "    pay_cols  = [f\"PAY_AMT{m}\"  for m in months]\n",
    "\n",
    "    bills = df[bill_cols].astype(float)\n",
    "    limit = df[\"LIMIT_BAL\"].astype(float).replace(0, np.nan)\n",
    "\n",
    "    # Utilization: bill / limit\n",
    "    util = bills.div(limit, axis=0).clip(0, 3.0).fillna(0.0)\n",
    "    out = pd.concat([out, _rowwise_stats(util, util.columns.tolist(), \"util\")], axis=1)\n",
    "\n",
    "    # Util slope with dynamic T\n",
    "    T = util.shape[1]\n",
    "    x_time = np.arange(1, T + 1, dtype=float)\n",
    "    out[\"util_slope\"] = _slope_over_time(util.values, x_time)\n",
    "\n",
    "    # Bill dynamics (diffs) on aligned months\n",
    "    bill_vals = bills.values\n",
    "    for k, v in _differences_stats(bill_vals, \"bill\").items():\n",
    "        out[k] = v\n",
    "\n",
    "    # ===== Payment ratios: PAY_AMTt / (BILL_AMTt + eps) computed month-by-month =====\n",
    "    eps = 1e-3\n",
    "    pay_ratio = pd.DataFrame(index=df.index)\n",
    "    for m in months:\n",
    "        num = df[f\"PAY_AMT{m}\"].astype(float)\n",
    "        den = df[f\"BILL_AMT{m}\"].astype(float).replace(0, np.nan)\n",
    "        pay_ratio[f\"payr_{m}\"] = (num / (den + eps)).clip(0, 1.5).fillna(0.0)\n",
    "\n",
    "    out = pd.concat([out, _rowwise_stats(pay_ratio, pay_ratio.columns.tolist(), \"payr\")], axis=1)\n",
    "    out[\"payr_slope\"] = _slope_over_time(pay_ratio.values, np.arange(1, len(months) + 1, dtype=float))\n",
    "\n",
    "    # ===== Shortfall per month: BILL_AMTt - PAY_AMTt (aligned) =====\n",
    "    shortfall = pd.DataFrame(index=df.index)\n",
    "    for m in months:\n",
    "        shortfall[f\"short_{m}\"] = (df[f\"BILL_AMT{m}\"].astype(float) - df[f\"PAY_AMT{m}\"].astype(float))\n",
    "    out = pd.concat([out, _rowwise_stats(shortfall, shortfall.columns.tolist(), \"short\")], axis=1)\n",
    "\n",
    "    # ===== PAY_* statuses (delinquency severity) =====\n",
    "    # Láº¥y cÃ¡c cá»™t PAY_* thá»±c sá»± tá»“n táº¡i, giá»¯ thá»© tá»± \"gáº§n nháº¥t trÆ°á»›c\": PAY_0, PAY_2, PAY_3, PAY_4, PAY_5, PAY_6\n",
    "    pay_status_order = [c for c in [\"PAY_0\",\"PAY_2\",\"PAY_3\",\"PAY_4\",\"PAY_5\",\"PAY_6\"] if c in df.columns]\n",
    "    if len(pay_status_order) > 0:\n",
    "        pays = df[pay_status_order].astype(float)\n",
    "        pays_pos = pays.clip(lower=0)\n",
    "        out[\"pay_recency\"] = df[\"PAY_0\"].astype(float) if \"PAY_0\" in df.columns else 0.0\n",
    "        out[\"pay_max\"]     = pays.max(axis=1)\n",
    "        out[\"pay_sum_pos\"] = (pays.values > 0).sum(axis=1)\n",
    "        out[\"pay_days_sum\"]= pays_pos.sum(axis=1)\n",
    "\n",
    "        # Weighted by recency (most recent bigger weight)\n",
    "        # VÃ­ dá»¥ cÃ³ K cá»™t -> weights = K, K-1, ..., 1\n",
    "        K = len(pay_status_order)\n",
    "        weights = np.arange(K, 0, -1, dtype=float)\n",
    "        out[\"pay_wsum\"] = (np.clip(pays.values, 0, None) * weights).sum(axis=1)\n",
    "    else:\n",
    "        out[[\"pay_recency\",\"pay_max\",\"pay_sum_pos\",\"pay_days_sum\",\"pay_wsum\"]] = 0.0\n",
    "\n",
    "    # Keep categorical raw\n",
    "    for c in [\"SEX\", \"EDUCATION\", \"MARRIAGE\"]:\n",
    "        if c in df.columns:\n",
    "            out[c] = df[c].astype(int)\n",
    "        else:\n",
    "            out[c] = 0\n",
    "\n",
    "    # Keep key originals (no data loss) â€” dÃ¹ng aligned months cho BILL/PAY gá»‘c\n",
    "    keep_cols = [\"LIMIT_BAL\"] + bill_cols + pay_cols + [c for c in [\"PAY_0\",\"PAY_2\",\"PAY_3\",\"PAY_4\",\"PAY_5\",\"PAY_6\"] if c in df.columns]\n",
    "    out = pd.concat([out, df[keep_cols].astype(float)], axis=1)\n",
    "\n",
    "    return out.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62d1e9e",
   "metadata": {},
   "source": [
    "Cell 6/11 â€” Load data + FE + matrix hoÃ¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f151f5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building featuresâ€¦\n",
      "Feature dim: 55 | Train: 23000 | Test: 7000\n",
      "Categorical cols: ['SEX', 'EDUCATION', 'MARRIAGE', 'AGE_BIN']\n"
     ]
    }
   ],
   "source": [
    "# ============================== CELL 6/11 (patched, NO FLOAT DOWNCAST): Load Data & Build FE ==============================\n",
    "\"\"\"Load CSVs, build FE, keep DataFrame end-to-end. No float downcast; only ensure categorical as ints for CatBoost.\"\"\"\n",
    "\n",
    "start_time = time.time()\n",
    "safe_makedirs(OUTDIR)\n",
    "\n",
    "train = pd.read_csv(TRAIN_CSV)\n",
    "test  = pd.read_csv(TEST_CSV)\n",
    "\n",
    "assert TARGET_COL in train.columns, f\"Missing target '{TARGET_COL}' in train.csv\"\n",
    "assert ID_COL in train.columns and ID_COL in test.columns, \"Missing ID column\"\n",
    "\n",
    "y = train[TARGET_COL].astype(int).values\n",
    "train_ids = train[ID_COL].values\n",
    "test_ids  = test[ID_COL].values\n",
    "\n",
    "print(\"Building featuresâ€¦\")\n",
    "train_fe = add_safe_features(train)\n",
    "test_fe  = add_safe_features(test)\n",
    "\n",
    "# Äáº£m báº£o test cÃ³ Ä‘á»§ & Ä‘Ãºng thá»© tá»± cá»™t nhÆ° train\n",
    "assert set(train_fe.columns) == set(test_fe.columns), \"Train/Test feature sets differ!\"\n",
    "test_fe = test_fe[train_fe.columns]\n",
    "\n",
    "# Khai bÃ¡o categorical theo tÃªn (LightGBM dÃ¹ng tÃªn; CatBoost dÃ¹ng index)\n",
    "# Náº¿u dá»± Ã¡n mÃ y cÃ³ set khÃ¡c, sá»­a list dÆ°á»›i cho Ä‘Ãºng:\n",
    "cat_cols = [c for c in [\"SEX\", \"EDUCATION\", \"MARRIAGE\", \"AGE_BIN\"] if c in train_fe.columns]\n",
    "\n",
    "# Chá»‰ Ã©p categorical vá» int (giá»¯ nguyÃªn float dtypes cho sá»‘ liá»‡u khÃ¡c)\n",
    "for c in cat_cols:\n",
    "    train_fe[c] = train_fe[c].astype(\"int64\")\n",
    "    test_fe[c]  = test_fe[c].astype(\"int64\")\n",
    "\n",
    "# GIá»® DataFrame, KHÃ”NG .values\n",
    "X_df      = train_fe.copy()\n",
    "X_test_df = test_fe.copy()\n",
    "\n",
    "# CatBoost cáº§n index vá»‹ trÃ­ cá»™t categorical\n",
    "cat_idx = [X_df.columns.get_loc(c) for c in cat_cols]\n",
    "\n",
    "# Safety checks kiá»ƒu \"reviewer khÃ³ tÃ­nh\"\n",
    "assert list(X_df.columns) == list(X_test_df.columns), \"TÃªn/thá»© tá»± cá»™t giá»¯a train/test KHÃ”NG khá»›p.\"\n",
    "for c in cat_cols:\n",
    "    assert pd.api.types.is_integer_dtype(X_df[c]), f\"Categorical '{c}' pháº£i lÃ  int\"\n",
    "\n",
    "print(f\"Feature dim: {X_df.shape[1]} | Train: {X_df.shape[0]} | Test: {X_test_df.shape[0]}\")\n",
    "print(f\"Categorical cols: {cat_cols}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1f8146",
   "metadata": {},
   "source": [
    "Cell 7/11 â€” Train LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "560ce143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LGB][seed 42] OOF AUC = 0.787201\n",
      "[LGB][seed 2021] OOF AUC = 0.783149\n",
      "[LGB][seed 7] OOF AUC = 0.785245\n",
      "[LGB][seed 99] OOF AUC = 0.781153\n",
      "[LGB][seed 1234] OOF AUC = 0.784869\n"
     ]
    }
   ],
   "source": [
    "# ============================== CELL 7/11 (patched): Train LightGBM ==============================\n",
    "\"\"\"Train LGBM with DataFrame + categorical by name (no float downcast).\"\"\"\n",
    "\n",
    "oof_store, test_store, auc_store = {}, {}, {}\n",
    "\n",
    "if lgb is None:\n",
    "    print(\"[LGB] Skipped (library not installed). pip install -U lightgbm\")\n",
    "else:\n",
    "    lgb_oofs, lgb_tests, lgb_aucs = [], [], []\n",
    "    for sd in SEEDS:\n",
    "        set_seed(sd)\n",
    "        params = dict(\n",
    "            objective=\"binary\",\n",
    "            learning_rate=0.05,\n",
    "            num_leaves=31,\n",
    "            max_depth=-1,\n",
    "            min_data_in_leaf=60,\n",
    "            feature_fraction=0.9,\n",
    "            bagging_fraction=0.9,\n",
    "            bagging_freq=1,\n",
    "            lambda_l1=0.0,\n",
    "            lambda_l2=10.0,\n",
    "            metric=\"auc\",\n",
    "            n_estimators=2000,\n",
    "            random_state=sd,\n",
    "            verbose=-1,\n",
    "        )\n",
    "        skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=sd)\n",
    "        oof_pred = np.zeros(len(X_df), dtype=np.float64)\n",
    "        tst_pred = np.zeros(len(X_test_df), dtype=np.float64)\n",
    "\n",
    "        for fold, (trn_idx, val_idx) in enumerate(skf.split(X_df, y), 1):\n",
    "            X_tr, y_tr = X_df.iloc[trn_idx], y[trn_idx]\n",
    "            X_va, y_va = X_df.iloc[val_idx], y[val_idx]\n",
    "\n",
    "            model = lgb.LGBMClassifier(**params)\n",
    "            model.fit(\n",
    "                X_tr, y_tr,\n",
    "                eval_set=[(X_va, y_va)],\n",
    "                eval_metric=\"auc\",\n",
    "                categorical_feature=cat_cols,  # by NAME\n",
    "                callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)],\n",
    "            )\n",
    "            oof_pred[val_idx] = model.predict_proba(X_va)[:, 1]\n",
    "            tst_pred += model.predict_proba(X_test_df)[:, 1] / N_FOLDS\n",
    "\n",
    "        lgb_oofs.append(oof_pred)\n",
    "        lgb_tests.append(tst_pred)\n",
    "        lgb_auc = auc(y, oof_pred)\n",
    "        lgb_aucs.append(lgb_auc)\n",
    "        print(f\"[LGB][seed {sd}] OOF AUC = {lgb_auc:.6f}\")\n",
    "\n",
    "    oof_store[\"lgb\"] = np.vstack(lgb_oofs).mean(axis=0)\n",
    "    test_store[\"lgb\"] = np.vstack(lgb_tests).mean(axis=0)\n",
    "    auc_store[\"lgb_mean_auc\"] = float(np.mean(lgb_aucs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f295aae",
   "metadata": {},
   "source": [
    "Cell 8/11 â€” Train XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40a37aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[XGB][seed 42] OOF AUC = 0.790678\n",
      "[XGB][seed 2021] OOF AUC = 0.789370\n",
      "[XGB][seed 7] OOF AUC = 0.789042\n",
      "[XGB][seed 99] OOF AUC = 0.788457\n",
      "[XGB][seed 1234] OOF AUC = 0.788073\n"
     ]
    }
   ],
   "source": [
    "# ============================== CELL 8/11 (patched): Train XGBoost via xgb.train + DMatrix ==============================\n",
    "\"\"\"XGBoost: dÃ¹ng low-level API (xgb.train) Ä‘á»ƒ tÆ°Æ¡ng thÃ­ch má»i version; cÃ³ early stopping á»•n Ä‘á»‹nh.\n",
    "Giá»¯ DataFrame end-to-end; KHÃ”NG downcast float.\"\"\"\n",
    "\n",
    "if xgb is None:\n",
    "    print(\"[XGB] Skipped (library not installed). pip install -U xgboost\")\n",
    "else:\n",
    "    xgb_oofs, xgb_tests, xgb_aucs = [], [], []\n",
    "    neg, pos = (y == 0).sum(), (y == 1).sum()\n",
    "    spw = neg / max(pos, 1)\n",
    "\n",
    "    # Tham sá»‘ cho xgb.train (tÃªn param theo dáº¡ng native)\n",
    "    base_params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"eta\": 0.05,\n",
    "        \"max_depth\": 4,\n",
    "        \"min_child_weight\": 5,\n",
    "        \"subsample\": 0.9,\n",
    "        \"colsample_bytree\": 0.9,\n",
    "        \"reg_lambda\": 2.0,\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"scale_pos_weight\": spw,\n",
    "        \"nthread\": -1,      # dÃ¹ng táº¥t cáº£ CPU\n",
    "    }\n",
    "\n",
    "    # Reuse DMatrix cho test Ä‘á»ƒ nhanh hÆ¡n\n",
    "    dtest_global = xgb.DMatrix(X_test_df)\n",
    "\n",
    "    for sd in SEEDS:\n",
    "        params = dict(base_params)\n",
    "        params[\"seed\"] = sd\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=sd)\n",
    "        oof_pred = np.zeros(len(X_df), dtype=np.float64)\n",
    "        tst_pred = np.zeros(len(X_test_df), dtype=np.float64)\n",
    "\n",
    "        for fold, (trn_idx, val_idx) in enumerate(skf.split(X_df, y), 1):\n",
    "            X_tr, y_tr = X_df.iloc[trn_idx], y[trn_idx]\n",
    "            X_va, y_va = X_df.iloc[val_idx], y[val_idx]\n",
    "\n",
    "            dtr = xgb.DMatrix(X_tr, label=y_tr)\n",
    "            dva = xgb.DMatrix(X_va, label=y_va)\n",
    "\n",
    "            booster = xgb.train(\n",
    "                params=params,\n",
    "                dtrain=dtr,\n",
    "                num_boost_round=5000,\n",
    "                evals=[(dva, \"valid\")],\n",
    "                early_stopping_rounds=200,\n",
    "                verbose_eval=False,   # náº¿u muá»‘n xem log, Ä‘á»•i thÃ nh 100 hay True\n",
    "            )\n",
    "\n",
    "            # Dá»± Ä‘oÃ¡n dÃ¹ng best_iteration (tÆ°Æ¡ng thÃ­ch nhiá»u version)\n",
    "            try:\n",
    "                val_pred = booster.predict(dva, iteration_range=(0, booster.best_iteration + 1))\n",
    "                test_pred = booster.predict(dtest_global, iteration_range=(0, booster.best_iteration + 1))\n",
    "            except Exception:\n",
    "                ntree = getattr(booster, \"best_ntree_limit\", None)\n",
    "                if ntree is None:\n",
    "                    ntree = getattr(booster, \"best_iteration\", 0) + 1\n",
    "                val_pred = booster.predict(dva, ntree_limit=ntree)\n",
    "                test_pred = booster.predict(dtest_global, ntree_limit=ntree)\n",
    "\n",
    "            oof_pred[val_idx] = val_pred\n",
    "            tst_pred += test_pred / N_FOLDS\n",
    "\n",
    "        xgb_oofs.append(oof_pred)\n",
    "        xgb_tests.append(tst_pred)\n",
    "        xgb_auc = auc(y, oof_pred)\n",
    "        xgb_aucs.append(xgb_auc)\n",
    "        print(f\"[XGB][seed {sd}] OOF AUC = {xgb_auc:.6f}\")\n",
    "\n",
    "    oof_store[\"xgb\"] = np.vstack(xgb_oofs).mean(axis=0)\n",
    "    test_store[\"xgb\"] = np.vstack(xgb_tests).mean(axis=0)\n",
    "    auc_store[\"xgb_mean_auc\"] = float(np.mean(xgb_aucs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f3c653",
   "metadata": {},
   "source": [
    "ðŸ” Thay CELL 9/11: Train CatBoost (cat_features báº±ng index, no float downcast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c21372f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CAT][seed 42] OOF AUC = 0.792105\n",
      "[CAT][seed 2021] OOF AUC = 0.790292\n",
      "[CAT][seed 7] OOF AUC = 0.791919\n",
      "[CAT][seed 99] OOF AUC = 0.791423\n",
      "[CAT][seed 1234] OOF AUC = 0.791576\n"
     ]
    }
   ],
   "source": [
    "# ============================== CELL 9/11 (patched): Train CatBoost ==============================\n",
    "\"\"\"Train CAT using Pool with cat_features by index; keep numeric floats as-is.\"\"\"\n",
    "\n",
    "if CatBoostClassifier is None:\n",
    "    print(\"[CAT] Skipped (library not installed). pip install -U catboost\")\n",
    "else:\n",
    "    cat_oofs, cat_tests, cat_aucs = [], [], []\n",
    "    for sd in SEEDS:\n",
    "        set_seed(sd)\n",
    "        params = dict(\n",
    "            loss_function=\"Logloss\",\n",
    "            eval_metric=\"AUC\",\n",
    "            depth=6,\n",
    "            learning_rate=0.05,\n",
    "            l2_leaf_reg=5.0,\n",
    "            random_seed=sd,\n",
    "            iterations=3000,\n",
    "            od_type=\"Iter\",\n",
    "            od_wait=300,\n",
    "            task_type=\"CPU\",\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=sd)\n",
    "        oof_pred = np.zeros(len(X_df), dtype=np.float64)\n",
    "        tst_pred = np.zeros(len(X_test_df), dtype=np.float64)\n",
    "\n",
    "        for fold, (trn_idx, val_idx) in enumerate(skf.split(X_df, y), 1):\n",
    "            X_tr, y_tr = X_df.iloc[trn_idx], y[trn_idx]\n",
    "            X_va, y_va = X_df.iloc[val_idx], y[val_idx]\n",
    "\n",
    "            train_pool = Pool(X_tr, y_tr, cat_features=cat_idx if len(cat_idx) else None)\n",
    "            valid_pool = Pool(X_va, y_va, cat_features=cat_idx if len(cat_idx) else None)\n",
    "\n",
    "            model = CatBoostClassifier(**params)\n",
    "            model.fit(train_pool, eval_set=valid_pool, use_best_model=True)\n",
    "\n",
    "            oof_pred[val_idx] = model.predict_proba(valid_pool)[:, 1]\n",
    "            tst_pred += model.predict_proba(Pool(X_test_df, cat_features=cat_idx if len(cat_idx) else None))[:, 1] / N_FOLDS\n",
    "\n",
    "        cat_oofs.append(oof_pred)\n",
    "        cat_tests.append(tst_pred)\n",
    "        cat_auc = auc(y, oof_pred)\n",
    "        cat_aucs.append(cat_auc)\n",
    "        print(f\"[CAT][seed {sd}] OOF AUC = {cat_auc:.6f}\")\n",
    "\n",
    "    oof_store[\"cat\"] = np.vstack(cat_oofs).mean(axis=0)\n",
    "    test_store[\"cat\"] = np.vstack(cat_tests).mean(axis=0)\n",
    "    auc_store[\"cat_mean_auc\"] = float(np.mean(cat_aucs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb94d9e",
   "metadata": {},
   "source": [
    "Cell 10/11 â€” Ensemble (rank-avg) + Stacking (Logistic) + Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54f17355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ensemble][RankAvg] OOF AUC = 0.792295\n",
      "[Stack][LR] OOF AUC = 0.792354\n",
      "[FINAL][RankAvg(base+stack)] OOF AUC = 0.792432\n"
     ]
    }
   ],
   "source": [
    "# ============================== CELL 10/11: Ensemble, Stack, Diagnostics ==============================\n",
    "\"\"\"Rank-averaging + simple logistic stacker for robustness. You may turn off stacker if you want.\"\"\"\n",
    "\n",
    "def rank_normalize(a: np.ndarray) -> np.ndarray:\n",
    "    s = pd.Series(a)\n",
    "    r = s.rank(method='average')\n",
    "    return ((r - 1) / (len(s) - 1)).values\n",
    "\n",
    "# Sanity: must have at least one base model\n",
    "assert len(test_store) > 0, \"No base model trained. Install missing libs or check earlier cells.\"\n",
    "\n",
    "base_keys = list(test_store.keys())\n",
    "oof_base = np.vstack([oof_store[k] for k in base_keys]).T\n",
    "TST_base = np.vstack([test_store[k] for k in base_keys]).T\n",
    "\n",
    "# Rank-average ensemble\n",
    "oof_rank = np.mean(np.column_stack([rank_normalize(oof_base[:, i]) for i in range(oof_base.shape[1])]), axis=1)\n",
    "tst_rank = np.mean(np.column_stack([rank_normalize(TST_base[:, i]) for i in range(TST_base.shape[1])]), axis=1)\n",
    "auc_rank = auc(y, oof_rank)\n",
    "print(f\"[Ensemble][RankAvg] OOF AUC = {auc_rank:.6f}\")\n",
    "\n",
    "# Simple stacker on base OOF\n",
    "scaler = StandardScaler()\n",
    "X_meta = scaler.fit_transform(oof_base)\n",
    "meta_clf = LogisticRegression(max_iter=2000)\n",
    "meta_clf.fit(X_meta, y)\n",
    "oof_stack = meta_clf.predict_proba(X_meta)[:, 1]\n",
    "tst_stack = meta_clf.predict_proba(scaler.transform(TST_base))[:, 1]\n",
    "auc_stack = auc(y, oof_stack)\n",
    "print(f\"[Stack][LR] OOF AUC = {auc_stack:.6f}\")\n",
    "\n",
    "# Final blend: rank-average over (base models + stacker)\n",
    "oof_all = np.column_stack([oof_base, oof_stack])\n",
    "tst_all = np.column_stack([TST_base, tst_stack])\n",
    "oof_final = np.mean(np.column_stack([rank_normalize(oof_all[:, i]) for i in range(oof_all.shape[1])]), axis=1)\n",
    "tst_final = np.mean(np.column_stack([rank_normalize(tst_all[:, i]) for i in range(tst_all.shape[1])]), axis=1)\n",
    "auc_final = auc(y, oof_final)\n",
    "print(f\"[FINAL][RankAvg(base+stack)] OOF AUC = {auc_final:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddff77f",
   "metadata": {},
   "source": [
    "Cell 11/11 â€” LÆ°u submission + OOF + meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d77181cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Submission saved to: ./outputs\\submission_20250826-155148.csv\n",
      "[OK] OOF diagnostics saved to: ./outputs\\oof_20250826-155148.csv\n",
      "Done in 33.85 min\n"
     ]
    }
   ],
   "source": [
    "# ============================== CELL 11/11: Save Submission & Artifacts ==============================\n",
    "\"\"\"Writes submission CSV, OOF diagnostics, and run metadata JSON into OUTDIR.\"\"\"\n",
    "\n",
    "meta = {\n",
    "    \"time\": timestamp(),\n",
    "    \"seeds\": SEEDS,\n",
    "    \"base_models\": list(test_store.keys()),\n",
    "    \"auc\": {**auc_store, \"ensemble_rank\": float(auc_rank), \"stack_lr\": float(auc_stack), \"final_rank\": float(auc_final)},\n",
    "    \"train_rows\": int(X.shape[0]),\n",
    "    \"test_rows\": int(X_test.shape[0]),\n",
    "    \"n_features\": int(X.shape[1]),\n",
    "    \"features\": fe_cols,\n",
    "}\n",
    "\n",
    "safe_makedirs(OUTDIR)\n",
    "with open(os.path.join(OUTDIR, f\"run_meta_{timestamp()}.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Submission file (Kaggle format)\n",
    "sub = pd.DataFrame({\n",
    "    \"ID\": test_ids,\n",
    "    \"default_payment_next_month\": tst_final.astype(float),\n",
    "})\n",
    "sub_path = os.path.join(OUTDIR, f\"submission_{timestamp()}.csv\")\n",
    "sub.sort_values(\"ID\").to_csv(sub_path, index=False)\n",
    "print(f\"[OK] Submission saved to: {sub_path}\")\n",
    "\n",
    "# OOF diagnostics (help you inspect blend stability)\n",
    "oof_df = pd.DataFrame({\n",
    "    \"ID\": train_ids,\n",
    "    \"y\": y,\n",
    "    **{f\"oof_{k}\": oof_store[k] for k in base_keys},\n",
    "    \"oof_rank\": oof_rank,\n",
    "    \"oof_stack\": oof_stack,\n",
    "    \"oof_final\": oof_final,\n",
    "})\n",
    "oof_path = os.path.join(OUTDIR, f\"oof_{timestamp()}.csv\")\n",
    "oof_df.to_csv(oof_path, index=False)\n",
    "print(f\"[OK] OOF diagnostics saved to: {oof_path}\")\n",
    "\n",
    "print(f\"Done in {(time.time()-start_time)/60:.2f} min\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
